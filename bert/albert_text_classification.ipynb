{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ilsilfverskiold/smaller-models-docs/blob/main/nlp/cook/fine-tune/albert_text_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZrl7EHuJUfG"
      },
      "source": [
        "# Text Classification with Transformers (ALBERT)\n",
        "\n",
        "This script helps you fine-tune a pre-trained model (ALBERT) and encoder model for text classification with a dataset from the HuggingFace.\n",
        "\n",
        "The use case uses binary classes to produce a model to identify clickbait versus factual content with the use of a synthetic dataset found [here](https://huggingface.co/datasets/ilsilfverskiold/clickbait_titles_synthetic_data). This script follows a tutorial that you can find here.\n",
        "\n",
        "You may use any encoder model such as BERT, RoBERTa and DeBERTa instead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NvzGqWjSW2sd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in c:\\users\\chisa\\miniconda3\\envs\\nomi3\\lib\\site-packages (2.20.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\chisa\\miniconda3\\envs\\nomi3\\lib\\site-packages (from datasets) (3.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\chisa\\miniconda3\\envs\\nomi3\\lib\\site-packages (from datasets) (1.24.3)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\chisa\\miniconda3\\envs\\nomi3\\lib\\site-packages (from datasets) (16.1.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in c:\\users\\chisa\\miniconda3\\envs\\nomi3\\lib\\site-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\chisa\\miniconda3\\envs\\nomi3\\lib\\site-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: pandas in c:\\users\\chisa\\miniconda3\\envs\\nomi3\\lib\\site-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.32.2 in c:\\users\\chisa\\miniconda3\\envs\\nomi3\\lib\\site-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\chisa\\miniconda3\\envs\\nomi3\\lib\\site-packages (from datasets) (4.66.4)\n",
            "Requirement already satisfied: xxhash in c:\\users\\chisa\\miniconda3\\envs\\nomi3\\lib\\site-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: multiprocess in c:\\users\\chisa\\miniconda3\\envs\\nomi3\\lib\\site-packages (from datasets) (0.70.14)\n",
            "Requirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in c:\\users\\chisa\\miniconda3\\envs\\nomi3\\lib\\site-packages (from datasets) (2024.5.0)\n",
            "Requirement already satisfied: aiohttp in c:\\users\\chisa\\miniconda3\\envs\\nomi3\\lib\\site-packages (from datasets) (3.8.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in c:\\users\\chisa\\miniconda3\\envs\\nomi3\\lib\\site-packages (from datasets) (0.23.4)\n",
            "Requirement already satisfied: packaging in c:\\users\\chisa\\miniconda3\\envs\\nomi3\\lib\\site-packages (from datasets) (23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\chisa\\miniconda3\\envs\\nomi3\\lib\\site-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\chisa\\miniconda3\\envs\\nomi3\\lib\\site-packages (from aiohttp->datasets) (22.1.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in c:\\users\\chisa\\miniconda3\\envs\\nomi3\\lib\\site-packages (from aiohttp->datasets) (2.0.4)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\chisa\\miniconda3\\envs\\nomi3\\lib\\site-packages (from aiohttp->datasets) (6.0.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\chisa\\miniconda3\\envs\\nomi3\\lib\\site-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\chisa\\miniconda3\\envs\\nomi3\\lib\\site-packages (from aiohttp->datasets) (1.8.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\chisa\\miniconda3\\envs\\nomi3\\lib\\site-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\chisa\\miniconda3\\envs\\nomi3\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\chisa\\miniconda3\\envs\\nomi3\\lib\\site-packages (from huggingface-hub>=0.21.2->datasets) (4.7.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\chisa\\miniconda3\\envs\\nomi3\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\chisa\\miniconda3\\envs\\nomi3\\lib\\site-packages (from requests>=2.32.2->datasets) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\chisa\\miniconda3\\envs\\nomi3\\lib\\site-packages (from requests>=2.32.2->datasets) (2023.7.22)\n",
            "Requirement already satisfied: colorama in c:\\users\\chisa\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\chisa\\miniconda3\\envs\\nomi3\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\chisa\\miniconda3\\envs\\nomi3\\lib\\site-packages (from pandas->datasets) (2022.7)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\chisa\\miniconda3\\envs\\nomi3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: accelerate in c:\\users\\chisa\\miniconda3\\envs\\nomi3\\lib\\site-packages (0.31.0)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\chisa\\miniconda3\\envs\\nomi3\\lib\\site-packages (from accelerate) (1.24.3)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\chisa\\miniconda3\\envs\\nomi3\\lib\\site-packages (from accelerate) (23.0)\n",
            "Requirement already satisfied: psutil in c:\\users\\chisa\\appdata\\roaming\\python\\python310\\site-packages (from accelerate) (5.9.4)\n",
            "Requirement already satisfied: pyyaml in c:\\users\\chisa\\miniconda3\\envs\\nomi3\\lib\\site-packages (from accelerate) (6.0)\n",
            "Requirement already satisfied: torch>=1.10.0 in c:\\users\\chisa\\miniconda3\\envs\\nomi3\\lib\\site-packages (from accelerate) (2.0.1)\n",
            "Requirement already satisfied: huggingface-hub in c:\\users\\chisa\\miniconda3\\envs\\nomi3\\lib\\site-packages (from accelerate) (0.23.4)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\chisa\\miniconda3\\envs\\nomi3\\lib\\site-packages (from accelerate) (0.4.3)\n",
            "Requirement already satisfied: filelock in c:\\users\\chisa\\miniconda3\\envs\\nomi3\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions in c:\\users\\chisa\\miniconda3\\envs\\nomi3\\lib\\site-packages (from torch>=1.10.0->accelerate) (4.7.1)\n",
            "Requirement already satisfied: sympy in c:\\users\\chisa\\miniconda3\\envs\\nomi3\\lib\\site-packages (from torch>=1.10.0->accelerate) (1.11.1)\n",
            "Requirement already satisfied: networkx in c:\\users\\chisa\\miniconda3\\envs\\nomi3\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\chisa\\miniconda3\\envs\\nomi3\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\chisa\\miniconda3\\envs\\nomi3\\lib\\site-packages (from huggingface-hub->accelerate) (2024.5.0)\n",
            "Requirement already satisfied: requests in c:\\users\\chisa\\miniconda3\\envs\\nomi3\\lib\\site-packages (from huggingface-hub->accelerate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\chisa\\miniconda3\\envs\\nomi3\\lib\\site-packages (from huggingface-hub->accelerate) (4.66.4)\n",
            "Requirement already satisfied: colorama in c:\\users\\chisa\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>=4.42.1->huggingface-hub->accelerate) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\chisa\\miniconda3\\envs\\nomi3\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\chisa\\miniconda3\\envs\\nomi3\\lib\\site-packages (from requests->huggingface-hub->accelerate) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\chisa\\miniconda3\\envs\\nomi3\\lib\\site-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\chisa\\miniconda3\\envs\\nomi3\\lib\\site-packages (from requests->huggingface-hub->accelerate) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\chisa\\miniconda3\\envs\\nomi3\\lib\\site-packages (from requests->huggingface-hub->accelerate) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in c:\\users\\chisa\\miniconda3\\envs\\nomi3\\lib\\site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: transformers in c:\\users\\chisa\\miniconda3\\envs\\nomi3\\lib\\site-packages (4.41.2)\n",
            "Requirement already satisfied: filelock in c:\\users\\chisa\\miniconda3\\envs\\nomi3\\lib\\site-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in c:\\users\\chisa\\miniconda3\\envs\\nomi3\\lib\\site-packages (from transformers) (0.23.4)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\chisa\\miniconda3\\envs\\nomi3\\lib\\site-packages (from transformers) (1.24.3)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\chisa\\miniconda3\\envs\\nomi3\\lib\\site-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\chisa\\miniconda3\\envs\\nomi3\\lib\\site-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\chisa\\miniconda3\\envs\\nomi3\\lib\\site-packages (from transformers) (2022.7.9)\n",
            "Requirement already satisfied: requests in c:\\users\\chisa\\miniconda3\\envs\\nomi3\\lib\\site-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\chisa\\miniconda3\\envs\\nomi3\\lib\\site-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\chisa\\miniconda3\\envs\\nomi3\\lib\\site-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\users\\chisa\\miniconda3\\envs\\nomi3\\lib\\site-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\chisa\\miniconda3\\envs\\nomi3\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2024.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\chisa\\miniconda3\\envs\\nomi3\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.7.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\chisa\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\chisa\\miniconda3\\envs\\nomi3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\chisa\\miniconda3\\envs\\nomi3\\lib\\site-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\chisa\\miniconda3\\envs\\nomi3\\lib\\site-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\chisa\\miniconda3\\envs\\nomi3\\lib\\site-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: huggingface_hub in c:\\users\\chisa\\miniconda3\\envs\\nomi3\\lib\\site-packages (0.23.4)\n",
            "Requirement already satisfied: filelock in c:\\users\\chisa\\miniconda3\\envs\\nomi3\\lib\\site-packages (from huggingface_hub) (3.9.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\chisa\\miniconda3\\envs\\nomi3\\lib\\site-packages (from huggingface_hub) (2024.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in c:\\users\\chisa\\miniconda3\\envs\\nomi3\\lib\\site-packages (from huggingface_hub) (23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\chisa\\miniconda3\\envs\\nomi3\\lib\\site-packages (from huggingface_hub) (6.0)\n",
            "Requirement already satisfied: requests in c:\\users\\chisa\\miniconda3\\envs\\nomi3\\lib\\site-packages (from huggingface_hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\chisa\\miniconda3\\envs\\nomi3\\lib\\site-packages (from huggingface_hub) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\chisa\\miniconda3\\envs\\nomi3\\lib\\site-packages (from huggingface_hub) (4.7.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\chisa\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>=4.42.1->huggingface_hub) (0.4.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\chisa\\miniconda3\\envs\\nomi3\\lib\\site-packages (from requests->huggingface_hub) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\chisa\\miniconda3\\envs\\nomi3\\lib\\site-packages (from requests->huggingface_hub) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\chisa\\miniconda3\\envs\\nomi3\\lib\\site-packages (from requests->huggingface_hub) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\chisa\\miniconda3\\envs\\nomi3\\lib\\site-packages (from requests->huggingface_hub) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U datasets\n",
        "!pip install -U accelerate\n",
        "!pip install -U transformers\n",
        "!pip install -U huggingface_hub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TB6zvHZFJFMJ"
      },
      "source": [
        "Import the dataset you'll be trainin on. This dataset has a 'text' field and a 'label' field. Be sure to tweak the script if you need to."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "shz4-RFOf41U"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['label', 'text', '__index_level_0__'],\n",
              "        num_rows: 6614\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['label', 'text', '__index_level_0__'],\n",
              "        num_rows: 350\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['label', 'text', '__index_level_0__'],\n",
              "        num_rows: 818\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset, DatasetDict\n",
        "\n",
        "dataset = load_dataset(\"ilsilfverskiold/clickbait_titles_synthetic_data\")\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWE97S_2I-tC"
      },
      "source": [
        "Decide on your pre-trained model along with your new model's name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "GAIpslnnf6O6"
      },
      "outputs": [],
      "source": [
        "model_name = \"albert/albert-base-v2\"\n",
        "your_path = 'classify-clickbait'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DXTBviEKBia"
      },
      "source": [
        "Look over your distribution of the labels (optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "nOvRUItff915"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Label Distribution: Counter({'Factual': 4186, 'Clickbait': 2428})\n",
            "Test Label Distribution: Counter({'Factual': 519, 'Clickbait': 299})\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "train_label_distribution = Counter(dataset['train']['label'])\n",
        "test_label_distribution = Counter(dataset['test']['label'])\n",
        "\n",
        "print(\"Training Label Distribution:\", train_label_distribution)\n",
        "print(\"Test Label Distribution:\", test_label_distribution)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZyWkHH0KIA_"
      },
      "source": [
        "Create a label encoder that converts categorical labels to a standardized numerical format. Labels in their original categorical form (e.g., 'clickbait', 'factual') need to be converted into numerical values so that they can be processed by the algorithms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "5PeJ0mwigBQ9"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "label_encoder.fit(dataset['train']['label'])\n",
        "\n",
        "def encode_labels(example):\n",
        "    return {'encoded_label': label_encoder.transform([example['label']])[0]}\n",
        "\n",
        "for split in dataset:\n",
        "    dataset[split] = dataset[split].map(encode_labels, batched=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfKX7el9LEsq"
      },
      "source": [
        "The id2label and label2id mappings in AutoConfig are used to inform the model of the specific label-to-ID mappings so we can get the actual label names rather than the numerical reps when we do inference with the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_ECd3GJNgG-g"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\chisa\\miniconda3\\envs\\nomi3\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ID to Label Mapping: {0: 'Clickbait', 1: 'Factual'}\n",
            "Label to ID Mapping: {'Clickbait': 0, 'Factual': 1}\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoConfig\n",
        "\n",
        "unique_labels = sorted(list(set(dataset['train']['label'])))\n",
        "id2label = {i: label for i, label in enumerate(unique_labels)}\n",
        "label2id = {label: i for i, label in enumerate(unique_labels)}\n",
        "\n",
        "config = AutoConfig.from_pretrained(model_name)\n",
        "config.id2label = id2label\n",
        "config.label2id = label2id\n",
        "\n",
        "# Verify the correct labels\n",
        "print(\"ID to Label Mapping:\", config.id2label)\n",
        "print(\"Label to ID Mapping:\", config.label2id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cTrvYVzLkoI"
      },
      "source": [
        "The provided code snippet is responsible for loading a tokenizer and a model from the Hugging Face Transformers library. Here we use ALBERT as a model, you can use AutoTokenizer and AutoModelForSequenceClassification if you want to use another model or it's specified tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "KJoTq1K8gwGV"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "88c626d6ea2349f0b912e7c71d029b34",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4d46427176ff4ef79dada268e233f9f9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "spiece.model:   0%|          | 0.00/760k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5b1d0ea9dc874a599b78aa01954277e7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.31M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8af8a79da1934cedbf89de19e7356ea3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/47.4M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert/albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AlbertForSequenceClassification, AlbertTokenizer\n",
        "\n",
        "tokenizer = AlbertTokenizer.from_pretrained(model_name)\n",
        "model = AlbertForSequenceClassification.from_pretrained(model_name, config=config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOghCGAdMdqk"
      },
      "source": [
        "This next function makes sure the text data is properly tokenized and labeled, preparing the dataset for efficient training of the transformer model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "N4VJZjwhg38G"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6db3ca4b94784361b3bf87a2b0855b1c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter:   0%|          | 0/6614 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "624c51bf64e44e92b33f538ae5552d73",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter:   0%|          | 0/350 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7e6f6eee82754b88821e1eb65299e68e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter:   0%|          | 0/818 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ff7eff70ffa4499eb65e87b41b537669",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/6614 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "93d9ef0f4f554864a341e5255c3a8014",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/350 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "71b1927df185418295f3343a5997bb96",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/818 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['label', 'text', '__index_level_0__', 'encoded_label', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 6614\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['label', 'text', '__index_level_0__', 'encoded_label', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 350\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['label', 'text', '__index_level_0__', 'encoded_label', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 818\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def filter_invalid_content(example):\n",
        "    return isinstance(example['text'], str)\n",
        "\n",
        "dataset = dataset.filter(filter_invalid_content, batched=False)\n",
        "\n",
        "def encode_data(batch):\n",
        "    tokenized_inputs = tokenizer(batch[\"text\"], padding=True, truncation=True, max_length=256)\n",
        "    tokenized_inputs[\"labels\"] = batch[\"encoded_label\"]\n",
        "    return tokenized_inputs\n",
        "\n",
        "dataset_encoded = dataset.map(encode_data, batched=True)\n",
        "dataset_encoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "iQb-rRu9g5lT"
      },
      "outputs": [],
      "source": [
        "dataset_encoded.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-G5h1ET3NPLl"
      },
      "source": [
        "The DataCollatorWithPadding ensures that all input sequences in a batch are padded to the same length, using the padding logic defined by the tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "xg6Rb7R-g7A-"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSctqANNNaAc"
      },
      "source": [
        "Next we'll set up LabelEncoder to encode labels and defines a function to compute per-label accuracy from a confusion matrix, providing label-specific accuracy metrics. I.e. when we train the model we want to see the accuracy metrics per label as well as the average metrics. This is more relevant if you have more than two labels, and one is underperforming."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "zgMLYb57g-Lo"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(unique_labels)\n",
        "\n",
        "def per_label_accuracy(y_true, y_pred, labels):\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "    correct_predictions = cm.diagonal()\n",
        "    label_totals = cm.sum(axis=1)\n",
        "    per_label_acc = np.divide(correct_predictions, label_totals, out=np.zeros_like(correct_predictions, dtype=float), where=label_totals != 0)\n",
        "    return dict(zip(labels, per_label_acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTeQeL8DN05z"
      },
      "source": [
        "Next we set up our compute metrics. Here I've set up several, but you may reduce them if needed be. You can read more on this metrics [here.](https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "HInT5WReNvTS"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "\n",
        "    decoded_labels = label_encoder.inverse_transform(labels)\n",
        "    decoded_preds = label_encoder.inverse_transform(preds)\n",
        "\n",
        "    precision = precision_score(decoded_labels, decoded_preds, average='weighted')\n",
        "    recall = recall_score(decoded_labels, decoded_preds, average='weighted')\n",
        "    f1 = f1_score(decoded_labels, decoded_preds, average='weighted')\n",
        "    acc = accuracy_score(decoded_labels, decoded_preds)\n",
        "\n",
        "    labels_list = list(label_encoder.classes_)\n",
        "    per_label_acc = per_label_accuracy(decoded_labels, decoded_preds, labels_list)\n",
        "\n",
        "    per_label_acc_metrics = {}\n",
        "    for label, accuracy in per_label_acc.items():\n",
        "        label_key = f\"accuracy_label_{label}\"\n",
        "        per_label_acc_metrics[label_key] = accuracy\n",
        "\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        **per_label_acc_metrics\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dv3scyLpORAz"
      },
      "source": [
        "Lastly, we set up our training metrics to train the model. I'm following the paper [\"How to Fine-Tune BERT for Text Classification?\"](https://arxiv.org/abs/1905.05583) on epochs, batch size and learning rate but do play around with it if you want to.\n",
        "\n",
        "When it is in training, be sure to look out for training loss and validation loss. Both should decrease consistently. If validation is increasing consistently you may be overfitting your model and you can try to decrease number of epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "yx_etZ24hBjt"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "comet_ml is installed but `COMET_API_KEY` is not set.\n",
            "c:\\Users\\chisa\\miniconda3\\envs\\nomi3\\lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "31f1f4e98fbe45bb98fc7aa4894495ba",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/621 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.708, 'grad_norm': 22.045427322387695, 'learning_rate': 4.0000000000000003e-07, 'epoch': 0.05}\n",
            "{'loss': 0.6921, 'grad_norm': 17.884979248046875, 'learning_rate': 8.000000000000001e-07, 'epoch': 0.1}\n",
            "{'loss': 0.5808, 'grad_norm': 16.13038444519043, 'learning_rate': 1.2000000000000002e-06, 'epoch': 0.14}\n",
            "{'loss': 0.4741, 'grad_norm': 11.123197555541992, 'learning_rate': 1.6000000000000001e-06, 'epoch': 0.19}\n",
            "{'loss': 0.3645, 'grad_norm': 11.568208694458008, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.24}\n",
            "{'loss': 0.2771, 'grad_norm': 10.421098709106445, 'learning_rate': 2.4000000000000003e-06, 'epoch': 0.29}\n",
            "{'loss': 0.1937, 'grad_norm': 4.976494789123535, 'learning_rate': 2.8000000000000003e-06, 'epoch': 0.34}\n"
          ]
        }
      ],
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=your_path,\n",
        "    num_train_epochs=3,\n",
        "    warmup_steps=500,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy='steps',\n",
        "    eval_steps=100,\n",
        "    learning_rate=2e-5,\n",
        "    save_steps=1000,\n",
        "    gradient_accumulation_steps=2\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset_encoded['train'],\n",
        "    eval_dataset=dataset_encoded['test'],\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7L3GQsHPDRL"
      },
      "source": [
        "Once you're finito, you can evaluate the results, save your model and the state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lCiYYKyqhFL0"
      },
      "outputs": [],
      "source": [
        "trainer.evaluate()\n",
        "trainer.save_model(your_path)\n",
        "trainer.save_state()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vim7CVwhPKY4"
      },
      "source": [
        "If you want to test it out, you can run the pipeline directly with the model. I just used some new example titles to see how it did."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTvsu7rOhYTL"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "pipe = pipeline('text-classification', model='classify-clickbait')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wmG1MgK1haml"
      },
      "outputs": [],
      "source": [
        "example_titles = [\n",
        "    \"The Controversial Truth about Tech Debt\",\n",
        "    \"A Comprehensive Guide for Getting Started with Hugging Face\",\n",
        "    \"OpenAI GPT-4o: The New Best AI Model in the World. Like in the Movies. For Free\",\n",
        "    \"GPT4 Omni — So much more than just a voice assistant\",\n",
        "    \"Building Vector Databases with FastAPI and ChromaDB\",\n",
        "    \"How Pieter Levels Makes (At Least) $210K a Month From His Laptop — With Zero Employees\",\n",
        "    \"Which Is Better: Teachers or AI in the Classroom?\",\n",
        "    \"How to Build Enterprise-Scale Generative AI Agents with AWS Bedrock: A Comprehensive Guide\",\n",
        "    \"The Best Way To Start Your One-Person Business\",\n",
        "]\n",
        "\n",
        "for title in example_titles:\n",
        "    result = pipe(title)\n",
        "    print(f\"Title: {title}\")\n",
        "    print(f\"Output: {result[0]['label']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bscp7lvwPW7N"
      },
      "source": [
        "If you're satisfied, you can log in to HuggingFace with a token (you'll find these in your account under Settings - make sure it has write access)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UHx9RGTYhfqB"
      },
      "outputs": [],
      "source": [
        "# !huggingface-cli login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P98pcG_LPhoV"
      },
      "source": [
        "Push the model with your new name for it. It usually just takes the name you set when you trained it so whatever you put here doesn't matter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVE_898bhhTR"
      },
      "outputs": [],
      "source": [
        "# tokenizer.push_to_hub(\"username/classify-clickbait\")\n",
        "# trainer.push_to_hub(\"username/classify-clickbait\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUokttBHPvoc"
      },
      "source": [
        "Now, you're done. You got your text classifier."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyN3C3XgYBmqGPgFOx823pPs",
      "gpuType": "L4",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
